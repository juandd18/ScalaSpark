package com.juandavid.prueba

import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.apache.spark.sql.{SQLContext, Row, DataFrame}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType}
import org.apache.spark.HashPartitioner
import org.apache.spark.storage.StorageLevel
import org.apache.spark.rdd.RDD
import java.util.Calendar
import java.util.Date
import java.text.SimpleDateFormat
import java.security.MessageDigest



object CdcTwo {
  
  def main(args: Array[String]) {
     
    
     
    val sparkConfiguration = new SparkConf();
    sparkConfiguration.setMaster("local[*]");
    sparkConfiguration.setAppName("test-spark-job");

    val sc: SparkContext = new SparkContext(sparkConfiguration)
    
    val sqlContext= new org.apache.spark.sql.SQLContext(sc)

    import sqlContext.implicits._
    
    //
    //val das_file = sc.textFile("hdfs:////Colombia/landing/sanatas/20160717/Das/*")
    
    val file = "C:\\Users\\daaguila\\Documents\\datos\\Maestros_completos\\Das\\*"
    val file_updated = "C:\\Users\\daaguila\\Documents\\datos\\Maestros_completos\\muestra-updatedDas.txt"
   
    val dasSchema = StructType(Array(
    StructField("severidad_cobol", IntegerType, true),
    StructField("ind_eliminado", StringType, true),
    StructField("checksum", StringType, true),
    StructField("fecha_novedad", StringType, true),
    StructField("cod_pais_dato", StringType, true),  
    StructField("cod_fuente_dato", IntegerType, true),
    StructField("tipo_id", StringType, true),
    StructField("num_id", StringType, true),
    StructField("nacionalidad", StringType, true),
    StructField("nombre", StringType, true),
    StructField("fecha_creacion", StringType, true),
    StructField("fecha_modificacion", StringType, true),
    StructField("ind_modificacion", StringType, true),
    StructField("ind_bloqueo", StringType, true),
    StructField("InicioVigencia", StringType, true),
    StructField("FinalVigencia", StringType, true)
    ))
    
     val dasSchemaDos = StructType(Array(
    StructField("severidad_cobol", IntegerType, true),
    StructField("ind_eliminado", StringType, true),
    StructField("checksum", StringType, true),
    StructField("fecha_novedad", StringType, true),
    StructField("cod_pais_dato", StringType, true),  
    StructField("cod_fuente_dato", IntegerType, true),
    StructField("tipo_id", StringType, true),
    StructField("num_id", StringType, true),
    StructField("nacionalidad", StringType, true),
    StructField("nombre", StringType, true),
    StructField("fecha_creacion", StringType, true),
    StructField("fecha_modificacion", StringType, true),
    StructField("ind_modificacion", StringType, true),
    StructField("ind_bloqueo", StringType, true),
    StructField("InicioVigencia", StringType, true),
    StructField("FinalVigencia", StringType, true),
    StructField("hash_key", StringType, true),
    StructField("hash_kdt", StringType, true),
    StructField("hash_all", StringType, true)
    ))
   
      
   val das_ini_df = sqlContext.read
    .format("com.databricks.spark.csv")
    .option("header", "false") // Use first line of all files as header
    .option("delimiter",";")
    //.option("charset","ISO-8859-1")
    .option("charset","ISO-8859-1")
    .option("nullValue","n")
    .schema(dasSchema)
    .load(file)
   
    val md5fun = md5sum(_)
    val sqlmd5 = udf(md5fun)
    
    val md5funall = md5sumAll(_)
    val sqlmd5All = udf(md5funall)
    
    val datefun = getDate(_,_)
    val sqldate = udf(datefun)
    
   val cols_all = array($"tipo_id",$"num_id",$"nacionalidad",$"nombre",
       $"fecha_creacion",$"fecha_modificacion",$"ind_modificacion",$"ind_bloqueo")
   
   val das_file_df = das_ini_df.withColumn("hash_key",sqlmd5($"tipo_id"+$"num_id"))
   .withColumn("hash_kdt", sqlmd5($"tipo_id"+$"num_id"))
   .withColumn("hash_all", sqlmd5All(cols_all))
   //das_df.printSchema()
   //das_df.show(20)
  
   val das_upd_df = sqlContext.read.format("com.databricks.spark.csv")
    .option("header", "false") // Use first line of all files as header
    .option("delimiter",";").option("charset","ISO-8859-1")
    .schema(dasSchema)
    .load(file_updated)
    
   val das_update_df = das_upd_df.withColumn("hash_key",sqlmd5($"tipo_id"+$"num_id"))
   .withColumn("hash_kdt", sqlmd5($"tipo_id"+$"num_id"))
   .withColumn("hash_all", sqlmd5All(cols_all))
    //das_updated_df.printSchema()
    
    val das_file_rdd = das_file_df.rdd.map(entry => (entry.getString(17), entry))
    val das_updated_rdd = das_update_df.rdd.map(entry => (entry.getString(17), entry))
    
    val new_das_temp = das_updated_rdd.subtractByKey(das_file_rdd).map(x => x._2) 
    val new_das_temp2 = sqlContext.createDataFrame(new_das_temp, dasSchemaDos)
    val new_das = new_das_temp2.withColumn("InicioVigencia",date_format(unix_timestamp($"fecha_novedad", "yyyyMMdd").cast("timestamp"), "yyyy-MM-dd"))
    //new_das.show();
    
     //hash_Key equals but select changes
   val joined = das_file_df.as("a").join(das_update_df.as("b"),$"a.hash_key" === $"b.hash_key" ,"inner")
   .where( $"a.hash_all" !== $"b.hash_all")
   //joined.show(20)
    
   //edit InicioVigencia for edit rows
   val joined_changes = joined
   .select( "b.*")
   .withColumn("InicioVigencia", date_format(unix_timestamp($"fecha_novedad", "yyyyMMdd").cast("timestamp"), "yyyy-MM-dd"))
   .withColumn("FinalVigencia", lit(null: String))
   //joined_changes.show(20)
   //joined_changes.printSchema()
   
   //edit Iniciovigencia for old rows and FinalVigencia
   val joined_old = joined
   .select("a.*").withColumn("InicioVigencia", 
       sqldate(date_format(unix_timestamp($"fecha_creacion", "yyyyMMdd").cast("timestamp"), "yyyy-MM-dd"),
       date_format(unix_timestamp($"fecha_modificacion", "yyyyMMdd").cast("timestamp"), "yyyy-MM-dd")))
   .withColumn("FinalVigencia", date_format(unix_timestamp($"fecha_novedad", "yyyyMMdd").cast("timestamp"), "yyyy-MM-dd"))
   //joined_old.show(20)
   //joined_old.printSchema()
   
   val all_joined = joined_changes.unionAll(joined_old)
   //all_joined.show(20)
   
   val das_union = das_file_df.except(all_joined).unionAll(all_joined).unionAll(new_das)
   das_union.show(20) 
   
   sc.stop()
  }
  
  def md5sum(text: String) : String = {
        val digest = MessageDigest.getInstance("MD5")
        return digest.digest(text.getBytes).map("%02x".format(_)).mkString
    }
  
  def md5sumAll(strings: Seq[Any]) : String = {
        val digest = MessageDigest.getInstance("MD5")
        val texto = strings.filter(_ != null).mkString("")
        return digest.digest(texto.getBytes).map("%02x".format(_)).mkString
    }
  
   def getDate(fecha_creacion: String, fecha_modificacion : String) : String = {
        val cal = Calendar.getInstance()
        val sdf = new SimpleDateFormat("yyyy-MM-dd")
        if(fecha_creacion == null ){
          val d1c = sdf.parse(fecha_modificacion)
          cal.setTime(d1c);
       }
       else{
         cal.setTime(sdf.parse(fecha_creacion))
        }
   
        return sdf.format(cal.getTime())
    }
  
}
